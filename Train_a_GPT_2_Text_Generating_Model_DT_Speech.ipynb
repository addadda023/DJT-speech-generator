{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Train a GPT-2 Text-Generating Model DT Speech",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/addadda023/DJT-speech-generator/blob/master/Train_a_GPT_2_Text_Generating_Model_DT_Speech.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3PRpa1mLpqI",
        "colab_type": "text"
      },
      "source": [
        "### Text generation using [GPT-2-simple](https://github.com/minimaxir/gpt-2-simple),  Python package that wraps existing model fine-tuning and generation scripts for OpenAI's GPT-2 text generation model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOKPaPMxlKRA",
        "colab_type": "text"
      },
      "source": [
        "Let's download the packages. Note tensorflow 1.x version is installed because the gpt2 package doesn't support 2.0 yet. This is also important to note if you want to deploy the model as docker image later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Evq-Ru9WKbUy",
        "colab_type": "code",
        "outputId": "35d16328-88b7-4028-a51b-754bf06d73b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "!pip install -q gpt-2-simple\n",
        "import gpt_2_simple as gpt2\n",
        "from datetime import datetime\n",
        "from google.colab import files"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJI5bsivmy7Q",
        "colab_type": "text"
      },
      "source": [
        "### Check GPU status"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRMhzd97L-8q",
        "colab_type": "text"
      },
      "source": [
        "Since GPU is strongly recommended, check the status of GPU. Remember to select GPU in Tuntime -> Change runtime type."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yiTT-WEgK8Mh",
        "colab_type": "code",
        "outputId": "567dba34-a23f-4d66-c7ab-9b9bf4cd2721",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "# Check which GPU is being run \n",
        "!nvidia-smi"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Nov 14 05:47:16 2019       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 430.50       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   47C    P8    10W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlwxuwDsm603",
        "colab_type": "text"
      },
      "source": [
        "### GPT-2 Model download"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACrM8jwkNPD1",
        "colab_type": "text"
      },
      "source": [
        "To train the model on new text, we need to download the GPT-2 model first. \n",
        "There are three released sizes of GPT-2:\n",
        "\n",
        "1. 124M (default): the \"small\" model, 500MB on disk.\n",
        "2. 355M: the \"medium\" model, 1.5GB on disk.\n",
        "3. 774M: the \"large\" model, cannot currently be finetuned with Colaboratory but can be used to generate text from the pretrained model.\n",
        "4. 1558M: the \"extra large\", true model. Will not work if a K80 GPU is attached to the notebook. (like 774M, it cannot be finetuned).\n",
        "\n",
        "This next cell downloads it from Google Cloud Storage and saves it in the Colaboratory VM at /models/<model_name>."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1xdlC9QLeO6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "1b56cb91-5eb8-4f73-b937-227f0cdefffa"
      },
      "source": [
        "gpt2.download_gpt2(model_name=\"124M\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fetching checkpoint: 1.05Mit [00:00, 258Mit/s]                                                      \n",
            "Fetching encoder.json: 1.05Mit [00:00, 126Mit/s]                                                    \n",
            "Fetching hparams.json: 1.05Mit [00:00, 482Mit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 498Mit [00:05, 97.1Mit/s]                                  \n",
            "Fetching model.ckpt.index: 1.05Mit [00:00, 273Mit/s]                                                \n",
            "Fetching model.ckpt.meta: 1.05Mit [00:00, 121Mit/s]                                                 \n",
            "Fetching vocab.bpe: 1.05Mit [00:00, 170Mit/s]                                                       \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHi-kjrOnCs3",
        "colab_type": "text"
      },
      "source": [
        "### Uploading/loading your input text file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ybcq3sHyOPFo",
        "colab_type": "text"
      },
      "source": [
        "The best way to get input text to-be-trained into the Colaboratory VM, and to get the trained model out of Colaboratory, is to route it through Google Drive first.\n",
        "\n",
        "Running this cell (which will only work in Colaboratory) will mount your personal Google Drive in the VM, which later cells can use to get data in/out. (it will ask for an auth code; that auth is not saved anywhere)\n",
        "\n",
        "Alternatively, you can directly upload the text file to the notebook sidebar top left if its less than **10MB**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgOb92siNgqR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "56d46e4e-926c-449c-9b87-dfbb3b12427a"
      },
      "source": [
        "gpt2.mount_gdrive()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvF6ltO1Xwkz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Check contents of google drive\n",
        "!ls \"/content/drive/My Drive\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEUVgIpzOQ6x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file_name = 'all_transcripts.txt'\n",
        "gpt2.copy_file_from_gdrive(file_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93Czgi3xnf-b",
        "colab_type": "text"
      },
      "source": [
        "### Fine tuning GPT2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lvTPKzRmtRE",
        "colab_type": "text"
      },
      "source": [
        "The next cell will start the finetuning of GPT-2. It creates a persistent TensorFlow session which stores the training config, then runs the training for the specified number of steps (to have the finetuning run indefinitely, set steps = -1).\n",
        "\n",
        "The model checkpoints will be saved in `/checkpoint/run1` by default. Make sure to change the `run_name` variable if you're training different versions. The checkpoints are saved every 500 steps (can be changed) and when the cell is stopped.\n",
        "\n",
        "The training might time out after a few hours, so make sure you end training and save the results so you don't lose them! You can simply stop the cell and it will auto-store the last checkpoint data. The model will serve from that last checkpoint.\n",
        "\n",
        "**NOTE:** If you want to rerun this cell, restart the VM first (Runtime -> Restart Runtime). You will need to rerun imports but not recopy files.\n",
        "\n",
        "Parameters for gpt2.finetune:\n",
        "\n",
        "* **restore_from:** Set to fresh to start training from the base GPT-2, or set to latest to restart training from an existing checkpoint.\n",
        "* **sample_every:** Number of steps to print example output.\n",
        "* **print_every:** Number of steps to print training progress.\n",
        "* **learning_rate:** Learning rate for the training. (default 1e-4, can lower to 1e-5 if you have `<`1MB input data)\n",
        "* **run_name:** Subfolder within checkpoint to save the model. This is useful if you want to work with multiple models (will also need to specify run_name when loading the model).\n",
        "* **overwrite:** Set to True if you want to continue finetuning an existing model (w/ restore_from='latest') without creating duplicate copies."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S362cP38sSOA",
        "colab_type": "text"
      },
      "source": [
        "The input used to finetune this model is speech transcript from Donald Trump's all political rallies since Oct 2015. The speech transcripts were scraped from FactBase. Note the text is being used purely for educational purpose."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rfu8wTTuZGt4",
        "colab_type": "code",
        "outputId": "06c1241b-7128-40d1-88d7-aeacff3732ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "sess = gpt2.start_tf_sess()\n",
        "\n",
        "gpt2.finetune(sess,\n",
        "              dataset=file_name,\n",
        "              model_name='124M',\n",
        "              steps=1000,\n",
        "              restore_from='fresh',\n",
        "              run_name='run1',\n",
        "              print_every=10,\n",
        "              sample_every=200,\n",
        "              save_every=200\n",
        "              )"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/gpt_2_simple/src/sample.py:17: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Loading checkpoint models/124M/model.ckpt\n",
            "INFO:tensorflow:Restoring parameters from models/124M/model.ckpt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading dataset...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r100%|██████████| 1/1 [00:03<00:00,  3.82s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dataset has 753122 tokens\n",
            "Training...\n",
            "[10 | 29.08] loss=2.75 avg=2.75\n",
            "[20 | 51.13] loss=2.80 avg=2.77\n",
            "[30 | 73.61] loss=2.70 avg=2.75\n",
            "[40 | 96.60] loss=2.56 avg=2.70\n",
            "[50 | 120.24] loss=2.65 avg=2.69\n",
            "[60 | 144.11] loss=2.56 avg=2.67\n",
            "[70 | 167.50] loss=2.57 avg=2.65\n",
            "[80 | 190.93] loss=2.84 avg=2.68\n",
            "[90 | 214.61] loss=2.66 avg=2.68\n",
            "[100 | 238.16] loss=2.38 avg=2.65\n",
            "[110 | 261.65] loss=2.62 avg=2.64\n",
            "[120 | 285.19] loss=2.44 avg=2.63\n",
            "[130 | 308.83] loss=2.60 avg=2.62\n",
            "[140 | 332.47] loss=2.52 avg=2.61\n",
            "[150 | 356.10] loss=2.49 avg=2.61\n",
            "[160 | 379.69] loss=2.32 avg=2.59\n",
            "[170 | 403.30] loss=2.38 avg=2.57\n",
            "[180 | 426.87] loss=2.48 avg=2.57\n",
            "[190 | 450.44] loss=2.51 avg=2.56\n",
            "[200 | 474.03] loss=2.38 avg=2.55\n",
            "Saving checkpoint/run1/model-200\n",
            "======== SAMPLE 1 ========\n",
            " on the job, really.\n",
            "This election is about jobs and jobs, and people are working. For the first time since Reagan, we've produced more product -- we're producing more. We're not exporting, not exporting at all. And now it's like all around the globe. That's a good thing, right? Because we are all looking at each other. Now they're not, but they were not looking for this whole thing. OK, so where do they go with that? It's not just a matter of people, it's also a matter of jobs, jobs are coming back.\n",
            "So the only thing that matters right now, the only thing that matters is jobs. And that's what we're doing. And I will always fight for the laws and the borders, and for the safety and the wealth of our country. And to keep the borders secure, the Congress must approve the wall.\n",
            "Congressional approval is the very first thing that needs to happen in order to get our country moving again. That would take time. It would take decades, but it would be incredible. And you would see what is going to happen. We are now the 10th most expensive country we had under one of them, and we are going to do everything in our power to stop that horrible, terrible immigration plan. It's the single biggest campaign by far in our history.\n",
            "And I will tell you, I went out and I came in as an outsider. I just came in at 4 or 5 in the morning. It was a new town. I didn't know it was open, but I was outside and I see a big building, I can see a lot of people. It's great, but they're doing nice jobs.\n",
            "And I also recognized the great, wonderful, hard-working American soldiers and heroes who came back home. And we're going to be very grateful. We'll be very grateful. [Audience Boos]\n",
            "And the good thing's a thing that happened a long time ago, folks. You know what it is, you come from someplace, I mean, where the people back here, are like maybe they're not the kind of people that could vote for me for another short period of time, probably 20 or 30 or 40 years.\n",
            "We have the longest border in the history of our country, and we're going to have a very good --\n",
            "They're going to have a great border. The last thing I want to do is have bad things happen to my car or to my house. But we have the greatest border, and we have the greatest -- So. We're going to build a wall. Just build a wall. This may be the greatest wall anywhere in the world.\n",
            "I mean, look, you said last Tuesday, the other day, the other day, I said I just think of it. I said it, I just think of how beautiful it is. And just think of this -- We've got to build a wall. This is going to be a long wall.\n",
            "[Audience chants Maxine Waters is bad] And we are doing it, OK. We are doing it, but we are doing it with good men and bad. We need every one of them in this country, and we will. They have to come up with a plan as fast as we can come up with a plan.\n",
            "And I'm very proud that we have the highest Democrat rating on the wall, and by the way, I was a Republican in the House. [Booing] And the fact is, for what it's worth, the Democrats are not going to be there for me. They have got to get this built. But that's OK.\n",
            "It's OK. So we need our heroes, our warriors. We need heroes like these. They've been great, we fought them all. They have fought us, so you have to let them fight you. It's going to be a lot of fun.\n",
            "Right? [Audience Boos] How would we like to have a hero, right? [Laughter]\n",
            "But we need heroes like these. They have done an incredible job, so you know what? This guy is a hero. But you have got to have a certain skill set -- You know, these guys, they have done a lot of things.\n",
            "Last week, I got along very well with the Hispanic community, but we need people from all backgrounds, all faiths, all walks of life, and all different races. People can't make this work.\n",
            "But it's nice. This was not a good race. I mean, by the way, there are a lot of Hispanic people out there, right?\n",
            "But I am not gonna -- it's hard. It's hard. It's hard, because they're not gonna -- no, no, but that's OK. You have to let them go out here and give them a lot of credit. I mean, there is a guy with the Border Patrol, and they\n",
            "\n",
            "[210 | 514.85] loss=2.58 avg=2.56\n",
            "[220 | 538.39] loss=2.10 avg=2.53\n",
            "[230 | 561.90] loss=2.50 avg=2.53\n",
            "[240 | 585.53] loss=2.22 avg=2.52\n",
            "[250 | 609.20] loss=2.07 avg=2.50\n",
            "[260 | 632.81] loss=2.54 avg=2.50\n",
            "[270 | 656.39] loss=2.08 avg=2.48\n",
            "[280 | 679.94] loss=2.12 avg=2.47\n",
            "[290 | 703.53] loss=2.24 avg=2.46\n",
            "[300 | 727.08] loss=1.90 avg=2.44\n",
            "[310 | 750.66] loss=2.46 avg=2.44\n",
            "[320 | 774.19] loss=2.11 avg=2.42\n",
            "[330 | 797.77] loss=2.15 avg=2.42\n",
            "[340 | 821.34] loss=2.29 avg=2.41\n",
            "[350 | 844.89] loss=1.95 avg=2.40\n",
            "[360 | 868.47] loss=2.05 avg=2.38\n",
            "[370 | 892.05] loss=2.14 avg=2.38\n",
            "[380 | 915.62] loss=2.14 avg=2.37\n",
            "[390 | 939.22] loss=2.19 avg=2.36\n",
            "[400 | 962.80] loss=2.10 avg=2.36\n",
            "Saving checkpoint/run1/model-400\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n",
            "======== SAMPLE 1 ========\n",
            "Audience laughs and chants \"No more\"! They can't believe it! I think they're happy that it's over, but that doesn't mean they're happy.\n",
            "I think they're happy because it's the greatest media in the history of the World Cup. But we're winning so much, so fast, and so important. So beautiful. In fact, we won a corner of the World Cup that wasn't beautiful. It was a corner of a corner badly. We won a corner. We're winning from there. We're winning from everywhere, we're winning from all over the world.\n",
            "And we've won so little that we're looking up right now at NBC. I can't believe they're doing this for the World Cup, right? Because NBC, they have a problem. [Audience Chants \"Virtuoso\"] It's a big problem. They want to be taken down. They never go down. They never go. I've never done this. They never go down. They never go down.\n",
            "I went to a few countries, and even with the tears coming out of my eyes, you wouldn't believe it, folks. They had tears from their tears when we won a corner, right? When the tears went down.\n",
            "I'd be on television at the studio, and they'd make me stand with a newspaper and I'd take up tears. I'd be on television saying, \"How did he win, Mr. President?\" We won, we won, we won.\n",
            "And then the cameras went back to tears, but they don't come back now because we know how they went down and how they won. So if they come back, and you're watching the way they're coming back from CNN all of the sudden they go and they're saying, \"Oh, oh, oh, yeah, don't forget, he was like perfect.\" And I think they're also telling this story from a different angle, because they're always reporting with me that the last time they were terrible again was during the debates.\n",
            "This is incredible. Remember it was debate four times. If I'd been wrong four times, people would have said, \"Oh, good, I was perfect.\" Look at that. Look at that. And then the next one, which was by far the most amazing debate, but they didn't come back.\n",
            "And on television, but on television, every time they came back, they came back horrible, horrible things that they did. [Audience laughs] They're all telling this story that they're going to the Olympics.\n",
            "You go to the Olympics. I mean, this is so amazing. I've been saying it for years. Do you ever notice that almost all the people watching tonight are watching the same thing they've been doing for the last three years, which is, you know, there's nothing new about it. It's just been a while. Nobody's seen something like it.\n",
            "Now you people are watching it. You have a man in South Carolina. You had a man in Virginia. You had a couple of people that weren't your average guys, right?\n",
            "Now you have some people that were tough as hell, but they didn't know what was going on. And on television, they're doing that all the time. I've been telling this story for 25 years. And I say, they'll say, \"Sir, he didn't fight like a good man.\" There they are.\n",
            "There they are. I won't say it. And I said it to somebody, and I said it to the president of South Carolina and the president of Virginia, both of whom I respected. And they both said, \"No. It can't be easy. You can tell. You've won two elections, not easy, I mean.\" So that's what I said.\n",
            "But you know what happened? And then it just started. And you know what's happening, too.\n",
            "On June 4th the NFL is scheduled to start making its first cuts. They will be made immediately. And I have a lot of people that want them. I'm sorry about this, but just in case, the NFL can't make games. They can't make games. And then, a day later, on the evening of June 6th, they're going to start doing those cuts immediately.\n",
            "But the fact is, this was a big deal. And I'll tell you. I'll tell you what. You have the greatest players in the world. Think of it. But in a year and a half, you'll have this massive, massive problem where you're saying horrible things.\n",
            "You know what it's like? It's like saying horrible things. And you know it's OK, you're the same player. That's what you want. You don't need anybody.\n",
            "So we're going to make our games great. You know, we did games with a lot of great players, I'll tell you. But we'll\n",
            "\n",
            "[410 | 1001.39] loss=2.10 avg=2.35\n",
            "[420 | 1025.01] loss=2.06 avg=2.34\n",
            "[430 | 1048.58] loss=2.07 avg=2.33\n",
            "[440 | 1072.16] loss=2.17 avg=2.33\n",
            "[450 | 1095.74] loss=2.25 avg=2.33\n",
            "[460 | 1119.29] loss=1.91 avg=2.31\n",
            "[470 | 1142.85] loss=1.81 avg=2.30\n",
            "[480 | 1166.46] loss=1.89 avg=2.29\n",
            "[490 | 1190.01] loss=2.25 avg=2.29\n",
            "[500 | 1213.59] loss=1.78 avg=2.28\n",
            "[510 | 1237.14] loss=2.19 avg=2.27\n",
            "[520 | 1260.73] loss=2.04 avg=2.27\n",
            "[530 | 1284.30] loss=1.71 avg=2.25\n",
            "[540 | 1307.87] loss=1.63 avg=2.24\n",
            "[550 | 1331.47] loss=1.56 avg=2.22\n",
            "[560 | 1355.06] loss=1.99 avg=2.22\n",
            "[570 | 1378.63] loss=2.05 avg=2.21\n",
            "[580 | 1402.22] loss=2.08 avg=2.21\n",
            "[590 | 1425.82] loss=1.74 avg=2.20\n",
            "[600 | 1449.43] loss=1.38 avg=2.18\n",
            "Saving checkpoint/run1/model-600\n",
            "======== SAMPLE 1 ========\n",
            " radical. It's a rigged system.\n",
            "It's rigged. And so, in a sense, it's going to be the election of the fake news, the fake news, just like they're doing right now. There's no -- so here's one. This is the election of fake news and the fake press. We want truthful reporting. We don't we want the fake -- that's what this people is lousy at both, not very good. I'll tell you one instance.\n",
            "And he's in the -- hey, you know who it's for? They want to write articles attacking me, attacking me for being a socialist or whether or not I'm really from the American Midwest or whether or not I'm really from the United States of America. They want to make fun of me for it, but then the media turns the other way, does their job, the Democrats... [Applause] They turn -- look, there is no way for me to say this and this because they're fake... [Booing]... and they treat me badly, but that's OK. But you see what's happened to the coverage of me and I, you know, I got a lot of praise for -- I have a lot of praise.\n",
            "And the newscast in the media because all of this, the newscast today was me. You know... [Audience Laughs] Remember? Well, you know, I'm not the only one that remembers me. I'm just, you know, going back. But you know, it happened to me the other day. And I'm very proud. I'm honored. So, we're in Las Vegas, and, you know, I was driving through there yesterday. It wasn't long but a long drive. I was on the way to work, I think getting back home.\n",
            "The thing was driving me crazy. It was like a train heading in at me. Just goes everywhere everywhere. It travels everywhere and then it disappears. It goes right back to a place I just left. It's gone forever. And I wasn't thrilled, I was on my way to California. I didn't feel in the mood for a great -- I figured I could just -- you know those days are over.\n",
            "It's very hard. Well, not for me. Now it's all over. It's all over. But all over again it's driving me crazy, and it's driving me crazy... [Applause] And then I see the headline. I see -- I'm in Las Vegas. I'm in the midst -- I'll never forget my first day at work, the people were down.\n",
            "And I remember working -- I remember I said, \"What are you doing here?\"\n",
            "I had the greatest body, right? But the people of New Mexico knew who I was. They came in to see my house, they said, \"Mr. President, what are you doing here?\"\n",
            "I was sweating. [Applause] I was sweating. [Applause] I'm just laying there, sweating. Just remember my first phrase. I'll never forget it. It begins with a D, just starts off with.\n",
            "That's it.\n",
            "It goes out and I'll never forget it.\n",
            "I got a D at 15 or 16 years old. \"How old is this rooming room? Mr. President, 16?\" I never heard that question before. Oh, that is going to be a very interesting answer. Is that the tallest building in the whole world in some foreign country? Yes, sir, it's the tallest building in the world now.\n",
            "We've had more people on our level in the history. We've had more rooms on the first floor.\n",
            "But it was actually the other day, but still, that building's the tallest building in the entire world, folks, right? [Applause] But I -- this was my first experience of being the President of the United States. In other words, it was the first time in 40 years that I've been in this position. And I'm just telling you today, I didn't know it was going to happen this quickly.\n",
            "And it started a little bit early. I said, \"We got to get this started quickly. How do you do this?\" And they said, it's called the Presidential seal. I mean, one of the characters in The Apprentice did it. They said the most presidential seal. I said, \"Huh?\" There was no Presidential seal. I've been doing this since the day that I was 13 years old.\n",
            "But it was this day and night. And, you know, the day after it started -- I'll never forget it because I don't even want to mention it -- I thought about it. I thought I was in trouble, I thought this was somebody that was going to -- that was going to say Donald Trump is the greatest. I said it. But the day went by it became very clear to me\n",
            "\n",
            "[610 | 1487.89] loss=2.11 avg=2.18\n",
            "[620 | 1511.52] loss=1.73 avg=2.17\n",
            "[630 | 1535.15] loss=1.93 avg=2.17\n",
            "[640 | 1558.74] loss=1.82 avg=2.16\n",
            "[650 | 1582.37] loss=1.93 avg=2.15\n",
            "[660 | 1605.94] loss=1.74 avg=2.15\n",
            "[670 | 1629.52] loss=1.80 avg=2.14\n",
            "[680 | 1653.11] loss=1.65 avg=2.13\n",
            "[690 | 1676.67] loss=1.82 avg=2.12\n",
            "[700 | 1700.26] loss=1.81 avg=2.12\n",
            "[710 | 1723.84] loss=1.79 avg=2.11\n",
            "[720 | 1747.45] loss=2.00 avg=2.11\n",
            "[730 | 1771.03] loss=1.41 avg=2.09\n",
            "[740 | 1794.66] loss=1.81 avg=2.09\n",
            "[750 | 1818.30] loss=1.51 avg=2.08\n",
            "[760 | 1841.97] loss=1.83 avg=2.07\n",
            "[770 | 1865.66] loss=1.12 avg=2.06\n",
            "[780 | 1889.36] loss=1.35 avg=2.04\n",
            "[790 | 1913.05] loss=1.42 avg=2.03\n",
            "[800 | 1936.70] loss=1.92 avg=2.03\n",
            "Saving checkpoint/run1/model-800\n",
            "======== SAMPLE 1 ========\n",
            ", you're right. This is your final check, and hopefully tomorrow it catches on fire.\n",
            "No, you know what, you're right. You'll see some little beauties, you'll see some beauties like this. It's going to go like this, sleepy Joe Biden won. [Audience applauds] Yes, you heard that right. We're thrilled to be joined by many terrific Indiana Republicans, including a man that's here to help me. He's a really great guy.\n",
            "He's your governor. I say that because he ran one of the most successful administrations ever. And you know what, I just came from saying one of the most successful administrations I've ever seen, and we're going to continue to put America first. I mean, we have the greatest economy that we've ever had. But we're going to keep America great.\n",
            "He's a fighter for you and he loves your state. I've known him my whole life. I just saw the news today. He's a really smart, great man. You know, he's a great man -- he's a Congressman. In fact, I'd say -- I can take him out to a park and have dinner.\n",
            "I mean, it is very exciting, very exciting. And a great friend of mine used to say, very simply, they cannot be trusted. And I said, you know what, I guess that's where they're at? They cannot be trusted anymore. In fact, I thought that was my enemy. Because they say, no, you cannot say that.\n",
            "But Pocahontas can. Pocahontas voted Democrat in the 2016 primary, and now she's supporting Trump. I'll tell you what, we had a great relationship. I'll tell you what, she's going to end up being the next governor of that state. But I'll tell you what, I've known him my whole life. I could tell you a thousand stories, but this is the man.\n",
            "He'll never let up. He's going to do unbelievably well, but he's going to protect you and you have a great person coming into our country. He just is. He's going to protect you. He's also going to put a stop to the horrible trade deals that are taking place all over the planet.\n",
            "So we're doing well, but we have a great person in here -- I said he was not great, but you know what, I just want to tell you he's a man of God and he loves this country, so I'm so thrilled to be here tonight, right? Because this is a guy who's going to say you can do what you want.\n",
            "You can do what you want. This is a man who is going to retire as your governor and he's going to do it quickly and very respectfully. There's nobody -- there's nobody like him. There hasn't been a Democrat governor or governor-elect in the history of our country. There aren't going to be any more.\n",
            "We have Democrats, we have Republicans, we don't have any more than they are. I said, how do you get a president when you're running against a Republican? Nobody's done better than us. I said, don't worry about it. So, what's the problem? I said, you know, one thing that's clear, we have one thing that we want to achieve and we have to achieve it quickly.\n",
            "The Democrats will never do that. They want to raise your taxes, liberal loopholes, open borders. You know, open borders. How about that? How about a wall? You know, they'd like -- they're not happy, but you're going to raise your taxes and they said, yes, we go through California, we go through Nevada, we go through -- they say open borders.\n",
            "We got to do it. They don't want to do it.\n",
            "We're going to go through -- you've heard that story. One of the things you need to know in life is when somebody is a star, they never get. They're always going to be. These are -- there's only one thing they'll never get: they have a magic wand. That's a beautiful thing, but the magic wand ends up being death. They're always going to have open borders.\n",
            "They're going to take people in. I mean, there are none. In fact, I'd go as far as saying the greatest movement in the history of our country over the last 30 years, the Republican Party, they're all going to become stars. But the Democrats, even the liberals, don't want to do that. They want to protect high taxes, empty promises, and Democrat judges.\n",
            "And that's called liberal sanctuary cities. The word sanctuary city means it -- it's a bad thing for our country. And that's why the Democrat Party has been totally unable to pass its agenda. Because they are too tied up in the Democrats' failed policy of not\n",
            "\n",
            "[810 | 1974.94] loss=1.55 avg=2.02\n",
            "[820 | 1998.61] loss=1.29 avg=2.01\n",
            "[830 | 2022.30] loss=1.34 avg=2.00\n",
            "[840 | 2046.00] loss=1.64 avg=1.99\n",
            "[850 | 2069.68] loss=1.84 avg=1.99\n",
            "[860 | 2093.31] loss=1.48 avg=1.98\n",
            "[870 | 2116.93] loss=1.30 avg=1.97\n",
            "[880 | 2140.58] loss=1.40 avg=1.96\n",
            "[890 | 2164.20] loss=1.14 avg=1.94\n",
            "[900 | 2187.83] loss=1.50 avg=1.94\n",
            "[910 | 2211.44] loss=1.41 avg=1.93\n",
            "[920 | 2235.06] loss=1.31 avg=1.92\n",
            "[930 | 2258.70] loss=1.29 avg=1.91\n",
            "[940 | 2282.36] loss=1.18 avg=1.89\n",
            "[950 | 2306.04] loss=1.24 avg=1.88\n",
            "[960 | 2329.72] loss=1.32 avg=1.87\n",
            "[970 | 2353.45] loss=1.29 avg=1.87\n",
            "[980 | 2377.14] loss=1.16 avg=1.85\n",
            "[990 | 2400.79] loss=1.53 avg=1.85\n",
            "[1000 | 2424.50] loss=1.61 avg=1.85\n",
            "Saving checkpoint/run1/model-1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8AXcW1pGjBt",
        "colab_type": "text"
      },
      "source": [
        "Remember to copy the last checkpoint to Google drive. You can then download the model from Google drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ED5em-E2GmKq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gpt2.copy_checkpoint_to_gdrive(run_name='run1')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzIs3qwzvlyc",
        "colab_type": "text"
      },
      "source": [
        "### Generate text from trained model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8im5rf13vHyN",
        "colab_type": "text"
      },
      "source": [
        "Use the generate command to generate a sample output. \n",
        "\n",
        "Helpful parameters for gpt2.generate:\n",
        "\n",
        "* **length:** Number of tokens to generate (default 1023, the maximum)\n",
        "* **temperature:** The higher the temperature, the crazier the text (default 0.7, recommended to keep between 0.7 and 1.0)\n",
        "* **top_k:** Limits the generated guesses to the top k guesses (default 0 which disables the behavior; if the generated output is super crazy, you may want to set top_k=40)\n",
        "* **top_p:** Nucleus sampling: limits the generated guesses to a cumulative probability. (gets good results on a dataset with top_p=0.9)\n",
        "* **truncate:** Truncates the input text until a given sequence, excluding that sequence (e.g. if `truncate='<|endoftext|>'`, the returned text will include everything before the first <|endoftext|>). It may be useful to combine this with a smaller length if the input texts are short. You can also use `'\\n'` to generate only 1 line of output.\n",
        "* **include_prefix:** If using truncate and `include_prefix=False`, the specified prefix will not be included in the returned text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMEe11T8tT1X",
        "colab_type": "code",
        "outputId": "c219dd9e-c2d7-45a6-8d4e-745eb0b0b74e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        }
      },
      "source": [
        "gpt2.generate(sess, run_name='run1',\n",
        "              length=1023,\n",
        "              prefix='Apple')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Apple's $10 billion a year, but it's peanuts. And it's peanuts.\n",
            "China, we always lose for a long period. They can't take $10 billion from us. We have to make a deal. Under the USMCA, which is a tremendous deal, we will also protect Medicare. We have to protect it. And I have to tell you, we have a great, great new governor.\n",
            "Governor Cuomo is going to be fantastic. Governor Cuomo. And I have to say it, we have great new governor of New York, and I like him. I have to say it. We have great new governor of Florida, and I have to say it again, great new governor of Florida, by the way.\n",
            "Great new governor of all people, just a great guy, he loves the people of Florida. And when I make a deal with a state, I want to make sure it's going to work with him. But he's a great guy, and he loves the people of Florida. And he's going to be fantastic.\n",
            "And we will always protect patients with pre-existing condition. Always. Always. They have been doing it for so many decades.\n",
            "And we're going to get it fixed. We're going to get it fixed. Republicans passed the biggest tax cuts in American history, and the biggest in American history, 20 years ago, by far, in the history of our country. And it's also in the history of many countries in the world, because you have not been able to get your money from the Democrats, because they treat us very badly.\n",
            "You have not been able to take care of your education, or your health care. You have not been able to take care of your retirement. Now, they'll be OK, but the Democrats, they treat us very good. [Audience member calls out We love you Donald]\n",
            "And they've done a great job, but the Democrats, they treat us very bad, and the things we're doing, they're going to treat us very tough.\n",
            "We're going to have a great tax reform. We're going to have a socialist takeover of health care, and we're going to get it done. And I'll tell you what, I think it's time, but we have to get rid of the individual mandate, the most unpopular thing.\n",
            "And if we don't, we're going to have a government takeover, and we're going to be controlled by the people. We have no choice, and that's why I got rid of it. I got rid of it. We have no choice. We're going to have socialism, and hopefully that'll be a slow, painful death.\n",
            "It's happening, and we're going to have it stopped. And I'm really proud of it, because I see so many people now, and I see so many people now coming from other countries, coming through our country, coming through merit, but we're going to have it stopped. We need people like this, and we need companies like Apple, they need people like this, because they can't get away.\n",
            "They need jobs, they're the ones, they're the ones that want to make things. And there's no reason for Apple, because they've done so well, they do have some problems, but they have no problems, and I think they're going to be fixed, and I think it's going to be fixed quickly.\n",
            "And if Apple can't do that, then so be it. But we have some companies, and we have some companies, like Foxconn, they did the damn thing, and they did some good work, and they're going to do well, and they're going to have a great product, and they're going to be very proud of, the new Apple's, which I love.\n",
            "OK, so Apple, great company, and I'm not even talking about Apple, because they've been so good, and they have a lot of people that I don't even know, but they've been so good. I don't even know if they're going to do well, and they're not doing too well. I mean, OK, so they're not doing too well. But they're doing very well. And I don't know if Apple, and I think they're going to be in good hands, and I don't even know if they're going to do that.\n",
            "And I don't know if they're going to -- it's another question, do they want to make a product and take away the genius, and the talent, and the money that went into making it, and the talent, and the money that went into making it. So it's a question of do they want to do that, and do they really like Steve Jobs, or do they really like Steve Jobs and just go out and make the product and take it away, and you know what?\n",
            "You know what I'm saying, and I think they\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNjfsQW0wmX3",
        "colab_type": "text"
      },
      "source": [
        "### Loading a pretrained model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4tUj8U7FzOK",
        "colab_type": "text"
      },
      "source": [
        "You can also load a different pretrained model and generate text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6d8b-6PHtXsN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sess = gpt2.start_tf_sess()\n",
        "gpt2.load_gpt2(sess, run_name='run2')\n",
        "gpt2.generate(sess, run_name='run2',\n",
        "              length=100,\n",
        "              prefix='Coca cola',\n",
        "              truncate='\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mt7xeOQzvD5s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "920007cd-39de-445a-ef2b-f4ed38045bff"
      },
      "source": [
        "gpt2.tf.__version__"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.15.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    }
  ]
}